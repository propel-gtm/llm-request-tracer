package main

import (
	"context"
	"fmt"
	"log"
	"time"

	"github.com/sashabaranov/go-openai"
	"gorm.io/driver/sqlite"
	"gorm.io/gorm"

	llmtracer "github.com/yourusername/llm-request-tracer"
	"github.com/yourusername/llm-request-tracer/adapters"
)

// Your existing AIConfig
type AIConfigImpl struct {
	OpenAIModel   string
	OpenAIClient  *openai.Client
	TokenTracker  *llmtracer.TokenTracker // Add this field
}

// Your function with minimal changes - just add tracking
func (c *AIConfigImpl) CallOpenAI(contextMessage, userMessage string) (string, error) {
	startTime := time.Now()

	message, err := c.OpenAIClient.CreateChatCompletion(context.Background(), openai.ChatCompletionRequest{
		Model: c.OpenAIModel,
		Messages: []openai.ChatCompletionMessage{
			{Role: openai.ChatMessageRoleSystem, Content: contextMessage},
			{Role: openai.ChatMessageRoleUser, Content: userMessage},
		},
		Temperature: 0,
		TopP:        1,
	})

	duration := time.Since(startTime)

	// Add this single line to track tokens
	if c.TokenTracker != nil && message.Usage.PromptTokens > 0 {
		c.TokenTracker.TrackWithContext(
			context.Background(),
			llmtracer.ProviderOpenAI,
			c.OpenAIModel,
			message.Usage.PromptTokens,
			message.Usage.CompletionTokens,
			duration,
			err,
		)
	}

	if err != nil {
		return "", err
	}

	return message.Choices[0].Message.Content, nil
}

func main() {
	// One-time setup
	db, err := gorm.Open(sqlite.Open("token_usage.db"), &gorm.Config{})
	if err != nil {
		log.Fatal(err)
	}

	storage, err := adapters.NewGormAdapter(db)
	if err != nil {
		log.Fatal(err)
	}

	tokenTracker := llmtracer.NewTokenTracker(storage)

	// Create your AI config
	aiConfig := &AIConfigImpl{
		OpenAIModel:  "gpt-3.5-turbo",
		OpenAIClient: openai.NewClient("your-api-key"),
		TokenTracker: tokenTracker,
	}

	// Use normally
	response, err := aiConfig.CallOpenAI("You are a helpful assistant.", "What is 2+2?")
	if err != nil {
		log.Fatal(err)
	}
	fmt.Printf("Response: %s\n", response)

	// Get token usage stats
	stats, err := tokenTracker.GetTokenStats(context.Background(), nil)
	if err != nil {
		log.Fatal(err)
	}

	fmt.Println("\n=== Token Usage Stats ===")
	for modelKey, stat := range stats {
		fmt.Printf("%s:\n", modelKey)
		fmt.Printf("  Requests: %d\n", stat.TotalRequests)
		fmt.Printf("  Input Tokens: %d\n", stat.InputTokens)
		fmt.Printf("  Output Tokens: %d\n", stat.OutputTokens)
		fmt.Printf("  Total Tokens: %d\n", stat.TotalTokens)
		if stat.ErrorCount > 0 {
			fmt.Printf("  Errors: %d\n", stat.ErrorCount)
		}
	}
}

// Even simpler: Just track tokens after any LLM call
func simpleExample() {
	// Setup tracker once
	db, _ := gorm.Open(sqlite.Open("tokens.db"), &gorm.Config{})
	storage, _ := adapters.NewGormAdapter(db)
	tracker := llmtracer.NewTokenTracker(storage)

	// After any LLM API call, just do:
	tracker.Track(llmtracer.ProviderOpenAI, "gpt-4", 100, 200)
	
	// Or use the ultra-simple helper:
	llmtracer.QuickTrack(tracker, llmtracer.ProviderOpenAI, "gpt-4", 150, 250)
}